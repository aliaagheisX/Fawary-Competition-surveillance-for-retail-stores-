{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T00:06:57.633357Z",
     "iopub.status.busy": "2025-03-12T00:06:57.629686Z",
     "iopub.status.idle": "2025-03-12T00:07:01.907424Z",
     "shell.execute_reply": "2025-03-12T00:07:01.906086Z",
     "shell.execute_reply.started": "2025-03-12T00:06:57.633304Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# ----------- local imports ----------- \n",
    "from utils import show_images\n",
    "from constants import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T00:07:01.910693Z",
     "iopub.status.busy": "2025-03-12T00:07:01.910194Z",
     "iopub.status.idle": "2025-03-12T00:07:04.602336Z",
     "shell.execute_reply": "2025-03-12T00:07:04.601269Z",
     "shell.execute_reply.started": "2025-03-12T00:07:01.910657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 10:42:36.898859: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-21 10:42:37.188584: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-21 10:42:37.264446: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-21 10:42:38.102581: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-21 10:42:41.337863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from deepface import DeepFace\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pickle Representation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person_0</td>\n",
       "      <td>[0.3298376335629395, -0.7333409530775887, -1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>person_1</td>\n",
       "      <td>[0.4783467225009395, 1.0117955845930884, -1.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>person_10</td>\n",
       "      <td>[0.4395070165395737, -0.2822106957435608, 0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>person_11</td>\n",
       "      <td>[0.6532818440061349, 0.564850529942375, -0.769...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person_12</td>\n",
       "      <td>[-0.2833601363003254, 0.6550906949987014, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>person_60</td>\n",
       "      <td>[0.636040463577956, 0.5885688029229641, -0.439...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>person_61</td>\n",
       "      <td>[0.03856503907987412, 0.8731057628830696, 0.20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>person_7</td>\n",
       "      <td>[0.39031277467705766, 0.3133518789153622, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>person_8</td>\n",
       "      <td>[0.01244738662526721, -0.8572551366828737, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>person_9</td>\n",
       "      <td>[-0.9962185777171895, 0.06009429251706159, 0.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       person                                          embedding\n",
       "0    person_0  [0.3298376335629395, -0.7333409530775887, -1.0...\n",
       "1    person_1  [0.4783467225009395, 1.0117955845930884, -1.16...\n",
       "2   person_10  [0.4395070165395737, -0.2822106957435608, 0.18...\n",
       "3   person_11  [0.6532818440061349, 0.564850529942375, -0.769...\n",
       "4   person_12  [-0.2833601363003254, 0.6550906949987014, 0.09...\n",
       "..        ...                                                ...\n",
       "57  person_60  [0.636040463577956, 0.5885688029229641, -0.439...\n",
       "58  person_61  [0.03856503907987412, 0.8731057628830696, 0.20...\n",
       "59   person_7  [0.39031277467705766, 0.3133518789153622, -0.0...\n",
       "60   person_8  [0.01244738662526721, -0.8572551366828737, 0.1...\n",
       "61   person_9  [-0.9962185777171895, 0.06009429251706159, 0.5...\n",
       "\n",
       "[62 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "datastore_path = DATA_DIR / \"face_identification/train/ds_model_facenet512_detector_opencv_aligned_normalization_base_expand_0.pkl\"\n",
    "with open(datastore_path, \"rb\") as f:\n",
    "        representations = pickle.load(f)\n",
    "\n",
    "# convert represtations to df        \n",
    "df_rep = pd.DataFrame(representations)\n",
    "# add person column\n",
    "df_rep['person'] = df_rep['identity'].apply(lambda x: Path(x).parent.name)\n",
    "# get average embedding foreach person\n",
    "df_avg_embedding = df_rep.groupby(\"person\", as_index=False).agg({\n",
    "    \"embedding\": lambda x: np.mean(np.stack(x), axis=0)  # Average embeddings\n",
    "})\n",
    "\n",
    "df_avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = DATA_DIR/\"face_identification/train\"\n",
    "test_img_path = DATA_DIR/\"face_identification/test\"\n",
    "train_small = DATA_DIR/\"face_identification/train_small\"\n",
    "\n",
    "def make_small_dataset():\n",
    "    train_small.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    for img_path in tqdm(train_img_path.rglob(\"*.jpg\")):\n",
    "        person_path = train_small / img_path.parent.name\n",
    "        person_path.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        new_img_path = person_path / img_path.name\n",
    "        \n",
    "        Image.open(img_path).resize((160, 160), Image.LANCZOS).save(new_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741812715.175794    2340 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-12 22:51:55.194458: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from deepface.modules import modeling, detection, preprocessing\n",
    "\n",
    "model = modeling.build_model(task=\"facial_recognition\", model_name=\"Facenet512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(batch_size=16):\n",
    "    paths = []\n",
    "    images = []\n",
    "    images_path = sorted(list(train_img_path.rglob(\"*.jpg\")))\n",
    "    total_len = len(images_path)\n",
    "    \n",
    "    for i, img_path in enumerate(tqdm(images_path)):\n",
    "        # ============= preprocess =============\n",
    "        img = np.array( Image.open(img_path) )\n",
    "        img = preprocessing.resize_image(\n",
    "            img=img,\n",
    "            target_size=(160, 160),\n",
    "        )\n",
    "        img = preprocessing.normalize_input(img=img, normalization=\"base\")\n",
    "        \n",
    "        paths.append(img_path)\n",
    "        images.append(img) # Load image and append it\n",
    "        if (i + 1) % batch_size == 0 or (i+1) == total_len:\n",
    "            yield (paths, np.array(images)[:,0,:,:])\n",
    "            images = []\n",
    "            paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044420971d9d4e39a63658f5583baa63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "total_embeddings = pd.DataFrame(columns=['path', 'embeddings'])\n",
    "\n",
    "for paths, images in load_batch():\n",
    "    embeddings = model.model(images).numpy()\n",
    "    \n",
    "    batch_df = pd.DataFrame({'path': paths, 'embeddings': list(embeddings)})\n",
    "    \n",
    "    total_embeddings = pd.concat([total_embeddings, batch_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/aliaagheis/projects/Fawary-Competition-s...</td>\n",
       "      <td>[1.006344, -1.4313573, -0.17667598, 0.19756395...</td>\n",
       "      <td>person_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/aliaagheis/projects/Fawary-Competition-s...</td>\n",
       "      <td>[0.4157936, -1.3333133, -0.51268137, 0.3930167...</td>\n",
       "      <td>person_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/aliaagheis/projects/Fawary-Competition-s...</td>\n",
       "      <td>[-0.3850316, -2.5415323, -1.176042, -0.8309456...</td>\n",
       "      <td>person_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/aliaagheis/projects/Fawary-Competition-s...</td>\n",
       "      <td>[1.1891283, -1.013693, -0.68593585, 0.8123388,...</td>\n",
       "      <td>person_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/aliaagheis/projects/Fawary-Competition-s...</td>\n",
       "      <td>[0.78686726, -1.4626352, -0.86144245, -0.09703...</td>\n",
       "      <td>person_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6823</th>\n",
       "      <td>/home/aliaagheis/projects/Fawary-Competition-s...</td>\n",
       "      <td>[1.3901302, -0.33843932, -0.7714985, 0.0276765...</td>\n",
       "      <td>person_99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6824</th>\n",
       "      <td>/home/aliaagheis/projects/Fawary-Competition-s...</td>\n",
       "      <td>[-0.45898774, 0.4042239, -0.70163524, 0.538473...</td>\n",
       "      <td>person_99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6825</th>\n",
       "      <td>/home/aliaagheis/projects/Fawary-Competition-s...</td>\n",
       "      <td>[0.45999855, 0.7937782, 1.2101912, 0.326266, 2...</td>\n",
       "      <td>person_99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>/home/aliaagheis/projects/Fawary-Competition-s...</td>\n",
       "      <td>[-0.13476726, 1.1266866, 0.6428942, 0.83492845...</td>\n",
       "      <td>person_99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6827</th>\n",
       "      <td>/home/aliaagheis/projects/Fawary-Competition-s...</td>\n",
       "      <td>[0.15916723, -0.247267, 0.6546566, -0.17154302...</td>\n",
       "      <td>person_99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6828 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  \\\n",
       "0     /home/aliaagheis/projects/Fawary-Competition-s...   \n",
       "1     /home/aliaagheis/projects/Fawary-Competition-s...   \n",
       "2     /home/aliaagheis/projects/Fawary-Competition-s...   \n",
       "3     /home/aliaagheis/projects/Fawary-Competition-s...   \n",
       "4     /home/aliaagheis/projects/Fawary-Competition-s...   \n",
       "...                                                 ...   \n",
       "6823  /home/aliaagheis/projects/Fawary-Competition-s...   \n",
       "6824  /home/aliaagheis/projects/Fawary-Competition-s...   \n",
       "6825  /home/aliaagheis/projects/Fawary-Competition-s...   \n",
       "6826  /home/aliaagheis/projects/Fawary-Competition-s...   \n",
       "6827  /home/aliaagheis/projects/Fawary-Competition-s...   \n",
       "\n",
       "                                             embeddings     person  \n",
       "0     [1.006344, -1.4313573, -0.17667598, 0.19756395...   person_0  \n",
       "1     [0.4157936, -1.3333133, -0.51268137, 0.3930167...   person_0  \n",
       "2     [-0.3850316, -2.5415323, -1.176042, -0.8309456...   person_0  \n",
       "3     [1.1891283, -1.013693, -0.68593585, 0.8123388,...   person_0  \n",
       "4     [0.78686726, -1.4626352, -0.86144245, -0.09703...   person_0  \n",
       "...                                                 ...        ...  \n",
       "6823  [1.3901302, -0.33843932, -0.7714985, 0.0276765...  person_99  \n",
       "6824  [-0.45898774, 0.4042239, -0.70163524, 0.538473...  person_99  \n",
       "6825  [0.45999855, 0.7937782, 1.2101912, 0.326266, 2...  person_99  \n",
       "6826  [-0.13476726, 1.1266866, 0.6428942, 0.83492845...  person_99  \n",
       "6827  [0.15916723, -0.247267, 0.6546566, -0.17154302...  person_99  \n",
       "\n",
       "[6828 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_embeddings['person'] = total_embeddings['path'].apply(lambda x: Path(x).parent.name)\n",
    "total_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 283)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_embeddings['person'].value_counts().min(), total_embeddings['person'].value_counts().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T00:27:33.573439Z",
     "iopub.status.busy": "2025-03-12T00:27:33.572392Z",
     "iopub.status.idle": "2025-03-12T00:27:33.719178Z",
     "shell.execute_reply": "2025-03-12T00:27:33.717207Z",
     "shell.execute_reply.started": "2025-03-12T00:27:33.573406Z"
    }
   },
   "outputs": [],
   "source": [
    "# # ds_model_facenet512_detector_opencv_aligned_normalization_base_expand_0.pkl\n",
    "# dfs = DeepFace.find(\n",
    "#   img_path = \"pp.jpg\", \n",
    "#   db_path = DATA_DIR / \"face_identification/train\", \n",
    "#   model_name = 'Facenet512',\n",
    "#   detector_backend='opencv',  # Try 'mtcnn', 'retinaface', etc.\n",
    "#   enforce_detection=False,\n",
    "#   refresh_database=False\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
